{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2 as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>Original_ID</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>BDI</th>\n",
       "      <th>AUDIT</th>\n",
       "      <th>EDUCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-001</td>\n",
       "      <td>10600</td>\n",
       "      <td>F</td>\n",
       "      <td>43</td>\n",
       "      <td>CTL</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>10601</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>CTL</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-003</td>\n",
       "      <td>10602</td>\n",
       "      <td>F</td>\n",
       "      <td>22</td>\n",
       "      <td>CTL</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-004</td>\n",
       "      <td>10603</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>CTL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-005</td>\n",
       "      <td>10604</td>\n",
       "      <td>M</td>\n",
       "      <td>31</td>\n",
       "      <td>CTL</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sub-006</td>\n",
       "      <td>10605</td>\n",
       "      <td>F</td>\n",
       "      <td>24</td>\n",
       "      <td>CTL</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sub-007</td>\n",
       "      <td>10606</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>ALC</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sub-008</td>\n",
       "      <td>10607</td>\n",
       "      <td>M</td>\n",
       "      <td>38</td>\n",
       "      <td>ALC</td>\n",
       "      <td>42</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sub-009</td>\n",
       "      <td>10608</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>ALC</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub-010</td>\n",
       "      <td>10609</td>\n",
       "      <td>F</td>\n",
       "      <td>54</td>\n",
       "      <td>ALC</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sub-011</td>\n",
       "      <td>10610</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>ALC</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sub-012</td>\n",
       "      <td>10611</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>ALC</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sub-013</td>\n",
       "      <td>10612</td>\n",
       "      <td>F</td>\n",
       "      <td>42</td>\n",
       "      <td>ALC</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sub-014</td>\n",
       "      <td>10613</td>\n",
       "      <td>M</td>\n",
       "      <td>33</td>\n",
       "      <td>ALC</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sub-015</td>\n",
       "      <td>10614</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>ALC</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sub-016</td>\n",
       "      <td>10615</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "      <td>ALC</td>\n",
       "      <td>40</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sub-017</td>\n",
       "      <td>10618</td>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "      <td>ALC</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sub-018</td>\n",
       "      <td>10619</td>\n",
       "      <td>F</td>\n",
       "      <td>44</td>\n",
       "      <td>ALC</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sub-019</td>\n",
       "      <td>10620</td>\n",
       "      <td>F</td>\n",
       "      <td>27</td>\n",
       "      <td>CTL</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sub-020</td>\n",
       "      <td>10621</td>\n",
       "      <td>F</td>\n",
       "      <td>34</td>\n",
       "      <td>CTL</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sub-021</td>\n",
       "      <td>10622</td>\n",
       "      <td>M</td>\n",
       "      <td>49</td>\n",
       "      <td>ALC</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sub-022</td>\n",
       "      <td>10623</td>\n",
       "      <td>F</td>\n",
       "      <td>49</td>\n",
       "      <td>ALC</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sub-023</td>\n",
       "      <td>10624</td>\n",
       "      <td>F</td>\n",
       "      <td>30</td>\n",
       "      <td>ALC</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sub-024</td>\n",
       "      <td>10625</td>\n",
       "      <td>F</td>\n",
       "      <td>29</td>\n",
       "      <td>ALC</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sub-025</td>\n",
       "      <td>10626</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>ALC</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sub-026</td>\n",
       "      <td>10627</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>ALC</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sub-027</td>\n",
       "      <td>10628</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>ALC</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sub-028</td>\n",
       "      <td>10629</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>ALC</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sub-029</td>\n",
       "      <td>10630</td>\n",
       "      <td>F</td>\n",
       "      <td>44</td>\n",
       "      <td>ALC</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sub-030</td>\n",
       "      <td>10631</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>ALC</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sub-031</td>\n",
       "      <td>10632</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>ALC</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sub-032</td>\n",
       "      <td>10633</td>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>ALC</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sub-033</td>\n",
       "      <td>10634</td>\n",
       "      <td>F</td>\n",
       "      <td>24</td>\n",
       "      <td>ALC</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sub-034</td>\n",
       "      <td>10635</td>\n",
       "      <td>F</td>\n",
       "      <td>34</td>\n",
       "      <td>ALC</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sub-035</td>\n",
       "      <td>10636</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>ALC</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>sub-036</td>\n",
       "      <td>10637</td>\n",
       "      <td>F</td>\n",
       "      <td>31</td>\n",
       "      <td>CTL</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sub-037</td>\n",
       "      <td>10638</td>\n",
       "      <td>F</td>\n",
       "      <td>34</td>\n",
       "      <td>CTL</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>sub-038</td>\n",
       "      <td>10639</td>\n",
       "      <td>F</td>\n",
       "      <td>24</td>\n",
       "      <td>ALC</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>sub-039</td>\n",
       "      <td>10640</td>\n",
       "      <td>M</td>\n",
       "      <td>22</td>\n",
       "      <td>CTL</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sub-040</td>\n",
       "      <td>10642</td>\n",
       "      <td>F</td>\n",
       "      <td>27</td>\n",
       "      <td>CTL</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sub-041</td>\n",
       "      <td>10643</td>\n",
       "      <td>M</td>\n",
       "      <td>33</td>\n",
       "      <td>CTL</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>sub-042</td>\n",
       "      <td>10644</td>\n",
       "      <td>F</td>\n",
       "      <td>26</td>\n",
       "      <td>CTL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sub-043</td>\n",
       "      <td>10645</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>CTL</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>sub-044</td>\n",
       "      <td>10646</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "      <td>CTL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>sub-045</td>\n",
       "      <td>10647</td>\n",
       "      <td>M</td>\n",
       "      <td>47</td>\n",
       "      <td>CTL</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>sub-046</td>\n",
       "      <td>10648</td>\n",
       "      <td>M</td>\n",
       "      <td>40</td>\n",
       "      <td>CTL</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>sub-047</td>\n",
       "      <td>10649</td>\n",
       "      <td>F</td>\n",
       "      <td>42</td>\n",
       "      <td>CTL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>sub-048</td>\n",
       "      <td>10650</td>\n",
       "      <td>M</td>\n",
       "      <td>48</td>\n",
       "      <td>CTL</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>sub-049</td>\n",
       "      <td>10651</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>CTL</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>sub-050</td>\n",
       "      <td>10652</td>\n",
       "      <td>M</td>\n",
       "      <td>43</td>\n",
       "      <td>CTL</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>sub-051</td>\n",
       "      <td>10653</td>\n",
       "      <td>F</td>\n",
       "      <td>53</td>\n",
       "      <td>CTL</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>sub-052</td>\n",
       "      <td>10656</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>CTL</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>sub-053</td>\n",
       "      <td>10657</td>\n",
       "      <td>M</td>\n",
       "      <td>47</td>\n",
       "      <td>CTL</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>sub-054</td>\n",
       "      <td>10658</td>\n",
       "      <td>M</td>\n",
       "      <td>52</td>\n",
       "      <td>CTL</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id  Original_ID sex  age GROUP  BDI  AUDIT  EDUCATION\n",
       "0         sub-001        10600   F   43   CTL   14      1         16\n",
       "1         sub-002        10601   F   50   CTL    2      2         16\n",
       "2         sub-003        10602   F   22   CTL    2      1         18\n",
       "3         sub-004        10603   F   18   CTL    0      1         13\n",
       "4         sub-005        10604   M   31   CTL    1      1         14\n",
       "5         sub-006        10605   F   24   CTL    8      3         19\n",
       "6         sub-007        10606   M   44   ALC    5      7         16\n",
       "7         sub-008        10607   M   38   ALC   42     27         13\n",
       "8         sub-009        10608   M   32   ALC    1     11         16\n",
       "9         sub-010        10609   F   54   ALC   17      9         12\n",
       "10        sub-011        10610   F   37   ALC    5     15         18\n",
       "11        sub-012        10611   M   51   ALC    1      7         16\n",
       "12        sub-013        10612   F   42   ALC    0      7         16\n",
       "13        sub-014        10613   M   33   ALC   24     19         18\n",
       "14        sub-015        10614   M   50   ALC    0      7         12\n",
       "15        sub-016        10615   M   28   ALC   40     21         16\n",
       "16        sub-017        10618   M   36   ALC    6     10         17\n",
       "17        sub-018        10619   F   44   ALC    0      5         10\n",
       "18        sub-019        10620   F   27   CTL    2      1         18\n",
       "19        sub-020        10621   F   34   CTL    1      1         16\n",
       "20        sub-021        10622   M   49   ALC   22     10         16\n",
       "21        sub-022        10623   F   49   ALC    3      3         12\n",
       "22        sub-023        10624   F   30   ALC   24     11         16\n",
       "23        sub-024        10625   F   29   ALC    5      6         12\n",
       "24        sub-025        10626   M   44   ALC    5      9         16\n",
       "25        sub-026        10627   F   28   ALC    0     13         13\n",
       "26        sub-027        10628   M   45   ALC   34     11         18\n",
       "27        sub-028        10629   F   37   ALC    4     12         18\n",
       "28        sub-029        10630   F   44   ALC    5     11         12\n",
       "29        sub-030        10631   F   45   ALC    8      7         17\n",
       "30        sub-031        10632   F   25   ALC   14      5         15\n",
       "31        sub-032        10633   F   55   ALC    6      5         12\n",
       "32        sub-033        10634   F   24   ALC    3     10         16\n",
       "33        sub-034        10635   F   34   ALC   17     20         16\n",
       "34        sub-035        10636   M   30   ALC   10     10         16\n",
       "35        sub-036        10637   F   31   CTL    4      1         22\n",
       "36        sub-037        10638   F   34   CTL    3      0         13\n",
       "37        sub-038        10639   F   24   ALC    8      5         16\n",
       "38        sub-039        10640   M   22   CTL    0      3         12\n",
       "39        sub-040        10642   F   27   CTL    2      0         17\n",
       "40        sub-041        10643   M   33   CTL    8      0         15\n",
       "41        sub-042        10644   F   26   CTL    0      1         21\n",
       "42        sub-043        10645   F   50   CTL   37      2         16\n",
       "43        sub-044        10646   F   32   CTL    0      1         14\n",
       "44        sub-045        10647   M   47   CTL    2      3         16\n",
       "45        sub-046        10648   M   40   CTL    2      0         16\n",
       "46        sub-047        10649   F   42   CTL    0      0         23\n",
       "47        sub-048        10650   M   48   CTL    2      1         16\n",
       "48        sub-049        10651   F   50   CTL    5      2         16\n",
       "49        sub-050        10652   M   43   CTL    0      2         18\n",
       "50        sub-051        10653   F   53   CTL    0      2         18\n",
       "51        sub-052        10656   M   44   CTL    7      0         12\n",
       "52        sub-053        10657   M   47   CTL   16      2         12\n",
       "53        sub-054        10658   M   52   CTL    1      2         12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participant = pd.read_csv(r'/Users/natchapon/Documents/data_sci/ds004515/participants.tsv' , sep='\\t')\n",
    "\n",
    "participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pg.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"eeg\",\n",
    "    user=\"admin\",\n",
    "    password=\"admin123\",\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     CTL\n",
       "1     CTL\n",
       "2     CTL\n",
       "3     CTL\n",
       "4     CTL\n",
       "5     CTL\n",
       "6     ALC\n",
       "7     ALC\n",
       "8     ALC\n",
       "9     ALC\n",
       "10    ALC\n",
       "11    ALC\n",
       "12    ALC\n",
       "13    ALC\n",
       "14    ALC\n",
       "15    ALC\n",
       "16    ALC\n",
       "17    ALC\n",
       "18    CTL\n",
       "19    CTL\n",
       "20    ALC\n",
       "21    ALC\n",
       "22    ALC\n",
       "23    ALC\n",
       "24    ALC\n",
       "25    ALC\n",
       "26    ALC\n",
       "27    ALC\n",
       "28    ALC\n",
       "29    ALC\n",
       "30    ALC\n",
       "31    ALC\n",
       "32    ALC\n",
       "33    ALC\n",
       "34    ALC\n",
       "35    CTL\n",
       "36    CTL\n",
       "37    ALC\n",
       "38    CTL\n",
       "39    CTL\n",
       "40    CTL\n",
       "41    CTL\n",
       "42    CTL\n",
       "43    CTL\n",
       "44    CTL\n",
       "45    CTL\n",
       "46    CTL\n",
       "47    CTL\n",
       "48    CTL\n",
       "49    CTL\n",
       "50    CTL\n",
       "51    CTL\n",
       "52    CTL\n",
       "53    CTL\n",
       "Name: GROUP, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "participant_id = participant['participant_id']\n",
    "participant_group = participant['GROUP']\n",
    "\n",
    "\n",
    "participant_id\n",
    "\n",
    "participant_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mne\n",
    "\n",
    "def insert_data_eeg(participant_id):\n",
    "    filePath = rf'/Users/natchapon/Documents/data_sci/ds004515/{participant_id}/eeg/{participant_id}_task-ProbabilisticSelection_eeg.set'\n",
    "    raw = mne.io.read_raw_eeglab(filePath)\n",
    "    eegData = raw.get_data()\n",
    "    ch_names = raw.ch_names\n",
    "\n",
    "    for i in range(len(ch_names)):\n",
    "        ch_name = ch_names[i]\n",
    "        ch_data = eegData[i]\n",
    "        name = ch_name.lower()\n",
    "        data = sum(ch_data) / len(ch_data)\n",
    "\n",
    "        if len(name.split(' ')) >= 2:\n",
    "            name = '_'.join(name.split(' '))\n",
    "        else:\n",
    "            name = name\n",
    "\n",
    "        sqlinsert = f\"INSERT INTO eeg_data (participant_id, {name}) VALUES ('{participant_id}', {data})\"\n",
    "        sqlupdate = f\"UPDATE eeg_data SET {name} = {data} WHERE participant_id = '{participant_id}'\"\n",
    "\n",
    "        if i == 0:\n",
    "            cur.execute(sqlinsert)\n",
    "            conn.commit()\n",
    "        else:\n",
    "            cur.execute(sqlupdate)\n",
    "            conn.commit()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-001/eeg/sub-001_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-002/eeg/sub-002_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-003/eeg/sub-003_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-004/eeg/sub-004_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting data for participant sub-004: column \"ft9\" of relation \"eeg_data\" does not exist\n",
      "LINE 1: UPDATE eeg_data SET ft9 = -0.010146454528895253 WHERE partic...\n",
      "                            ^\n",
      "\n",
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-005/eeg/sub-005_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting data for participant sub-005: column \"ft9\" of relation \"eeg_data\" does not exist\n",
      "LINE 1: UPDATE eeg_data SET ft9 = -0.024506568563852582 WHERE partic...\n",
      "                            ^\n",
      "\n",
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-006/eeg/sub-006_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-007/eeg/sub-007_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-008/eeg/sub-008_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-009/eeg/sub-009_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-010/eeg/sub-010_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-011/eeg/sub-011_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-012/eeg/sub-012_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-013/eeg/sub-013_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-014/eeg/sub-014_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-015/eeg/sub-015_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-016/eeg/sub-016_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-017/eeg/sub-017_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-018/eeg/sub-018_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-019/eeg/sub-019_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-020/eeg/sub-020_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-021/eeg/sub-021_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting data for participant sub-021: column \"ft9\" of relation \"eeg_data\" does not exist\n",
      "LINE 1: UPDATE eeg_data SET ft9 = -0.0016845392495547186 WHERE parti...\n",
      "                            ^\n",
      "\n",
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-022/eeg/sub-022_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting data for participant sub-022: column \"ft9\" of relation \"eeg_data\" does not exist\n",
      "LINE 1: UPDATE eeg_data SET ft9 = -0.011076317632348013 WHERE partic...\n",
      "                            ^\n",
      "\n",
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-023/eeg/sub-023_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting data for participant sub-023: column \"ft9\" of relation \"eeg_data\" does not exist\n",
      "LINE 1: UPDATE eeg_data SET ft9 = -0.016077003355977913 WHERE partic...\n",
      "                            ^\n",
      "\n",
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-024/eeg/sub-024_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting data for participant sub-024: column \"ft9\" of relation \"eeg_data\" does not exist\n",
      "LINE 1: UPDATE eeg_data SET ft9 = 0.016795412030821556 WHERE partici...\n",
      "                            ^\n",
      "\n",
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-025/eeg/sub-025_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-026/eeg/sub-026_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-027/eeg/sub-027_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-028/eeg/sub-028_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-029/eeg/sub-029_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-030/eeg/sub-030_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-031/eeg/sub-031_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-032/eeg/sub-032_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-033/eeg/sub-033_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-034/eeg/sub-034_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-035/eeg/sub-035_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-036/eeg/sub-036_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-037/eeg/sub-037_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-038/eeg/sub-038_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-039/eeg/sub-039_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-040/eeg/sub-040_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-041/eeg/sub-041_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-042/eeg/sub-042_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-043/eeg/sub-043_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-044/eeg/sub-044_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-045/eeg/sub-045_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-046/eeg/sub-046_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-047/eeg/sub-047_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-048/eeg/sub-048_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-049/eeg/sub-049_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-050/eeg/sub-050_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-051/eeg/sub-051_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-052/eeg/sub-052_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-053/eeg/sub-053_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/natchapon/Documents/data_sci/ds004515/sub-054/eeg/sub-054_task-ProbabilisticSelection_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1784186673.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(filePath)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(participant_id)):\n",
    "    try:\n",
    "        insert_data_eeg(participant_id[i])\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data for participant {participant_id[i]}: {e}\")\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/3291162496.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  rawDataSQl = pd.read_sql_query(\"SELECT * FROM eeg_data\", conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>fp1</th>\n",
       "      <th>fz</th>\n",
       "      <th>f3</th>\n",
       "      <th>f7</th>\n",
       "      <th>below_eye</th>\n",
       "      <th>fc5</th>\n",
       "      <th>fc1</th>\n",
       "      <th>c3</th>\n",
       "      <th>t7</th>\n",
       "      <th>...</th>\n",
       "      <th>c2</th>\n",
       "      <th>fc4</th>\n",
       "      <th>ft8</th>\n",
       "      <th>f6</th>\n",
       "      <th>f2</th>\n",
       "      <th>af4</th>\n",
       "      <th>af8</th>\n",
       "      <th>empty</th>\n",
       "      <th>ekg</th>\n",
       "      <th>audiooutput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-040</td>\n",
       "      <td>-0.003777</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>-0.012560</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>-0.014736</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>-0.002208</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>-1.346364</td>\n",
       "      <td>0.002399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-038</td>\n",
       "      <td>-0.033660</td>\n",
       "      <td>-0.001765</td>\n",
       "      <td>-0.012161</td>\n",
       "      <td>-0.019336</td>\n",
       "      <td>-0.037943</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.008832</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>-0.005543</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007977</td>\n",
       "      <td>-0.016760</td>\n",
       "      <td>-0.015461</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>-0.005819</td>\n",
       "      <td>-0.019429</td>\n",
       "      <td>-0.017650</td>\n",
       "      <td>0.012533</td>\n",
       "      <td>-0.015123</td>\n",
       "      <td>0.002376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-042</td>\n",
       "      <td>-0.022187</td>\n",
       "      <td>-0.018431</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.017319</td>\n",
       "      <td>-0.014809</td>\n",
       "      <td>-0.001071</td>\n",
       "      <td>-0.009401</td>\n",
       "      <td>-0.000777</td>\n",
       "      <td>-0.000968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>-0.009453</td>\n",
       "      <td>-0.011647</td>\n",
       "      <td>-0.010740</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>-0.006930</td>\n",
       "      <td>-0.004330</td>\n",
       "      <td>0.012517</td>\n",
       "      <td>-0.447032</td>\n",
       "      <td>0.002381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-046</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>0.007892</td>\n",
       "      <td>-0.005101</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>-0.004049</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>-0.004978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005497</td>\n",
       "      <td>0.021531</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>-0.000772</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>-0.003559</td>\n",
       "      <td>0.012418</td>\n",
       "      <td>-0.716166</td>\n",
       "      <td>0.002380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-043</td>\n",
       "      <td>-0.007540</td>\n",
       "      <td>0.011554</td>\n",
       "      <td>-0.003990</td>\n",
       "      <td>-0.001310</td>\n",
       "      <td>-0.007328</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>-0.000780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.007011</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>-0.004701</td>\n",
       "      <td>-0.013563</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>-1.083249</td>\n",
       "      <td>0.002370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sub-041</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>-0.009281</td>\n",
       "      <td>-0.003000</td>\n",
       "      <td>-0.015570</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.003218</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>-0.014805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>-0.002724</td>\n",
       "      <td>-0.008276</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>-0.008720</td>\n",
       "      <td>-0.021106</td>\n",
       "      <td>0.012515</td>\n",
       "      <td>-0.490343</td>\n",
       "      <td>0.002397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sub-036</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>-0.005508</td>\n",
       "      <td>-0.012362</td>\n",
       "      <td>-0.011743</td>\n",
       "      <td>-0.003125</td>\n",
       "      <td>-0.002425</td>\n",
       "      <td>-0.002463</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>-0.012248</td>\n",
       "      <td>-0.019721</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>-0.007371</td>\n",
       "      <td>-0.003281</td>\n",
       "      <td>-0.012928</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>0.728034</td>\n",
       "      <td>0.002414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sub-037</td>\n",
       "      <td>-0.022464</td>\n",
       "      <td>-0.022063</td>\n",
       "      <td>-0.017743</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.018002</td>\n",
       "      <td>-0.010503</td>\n",
       "      <td>-0.015123</td>\n",
       "      <td>-0.007808</td>\n",
       "      <td>-0.015364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>-0.016360</td>\n",
       "      <td>-0.015877</td>\n",
       "      <td>-0.010257</td>\n",
       "      <td>-0.019731</td>\n",
       "      <td>-0.017743</td>\n",
       "      <td>-0.019444</td>\n",
       "      <td>0.012538</td>\n",
       "      <td>0.078153</td>\n",
       "      <td>0.002373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sub-039</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>0.025463</td>\n",
       "      <td>0.011351</td>\n",
       "      <td>0.028762</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.026632</td>\n",
       "      <td>0.020138</td>\n",
       "      <td>0.010270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010249</td>\n",
       "      <td>0.015135</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.020085</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>0.029482</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>-0.207660</td>\n",
       "      <td>0.002457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub-045</td>\n",
       "      <td>0.020870</td>\n",
       "      <td>0.023257</td>\n",
       "      <td>0.013297</td>\n",
       "      <td>0.026966</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.029131</td>\n",
       "      <td>0.019025</td>\n",
       "      <td>0.018251</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028374</td>\n",
       "      <td>0.011032</td>\n",
       "      <td>-0.005392</td>\n",
       "      <td>0.023527</td>\n",
       "      <td>0.023697</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>-0.413792</td>\n",
       "      <td>0.002360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sub-047</td>\n",
       "      <td>-0.011584</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>-0.005707</td>\n",
       "      <td>-0.002414</td>\n",
       "      <td>-0.018764</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.010615</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>-0.003719</td>\n",
       "      <td>-0.023669</td>\n",
       "      <td>0.012729</td>\n",
       "      <td>0.015950</td>\n",
       "      <td>0.002386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sub-044</td>\n",
       "      <td>-0.005708</td>\n",
       "      <td>-0.003644</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>-0.001842</td>\n",
       "      <td>-0.017365</td>\n",
       "      <td>-0.005644</td>\n",
       "      <td>-0.009976</td>\n",
       "      <td>-0.006106</td>\n",
       "      <td>-0.003016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007209</td>\n",
       "      <td>-0.009449</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>-0.010831</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>-0.017923</td>\n",
       "      <td>-0.008670</td>\n",
       "      <td>0.012566</td>\n",
       "      <td>0.065308</td>\n",
       "      <td>0.002373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sub-048</td>\n",
       "      <td>-0.006969</td>\n",
       "      <td>-0.010642</td>\n",
       "      <td>-0.012793</td>\n",
       "      <td>-0.025560</td>\n",
       "      <td>-0.015865</td>\n",
       "      <td>-0.026004</td>\n",
       "      <td>-0.021883</td>\n",
       "      <td>-0.007552</td>\n",
       "      <td>-0.024513</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006648</td>\n",
       "      <td>-0.006894</td>\n",
       "      <td>-0.017713</td>\n",
       "      <td>-0.017027</td>\n",
       "      <td>-0.005503</td>\n",
       "      <td>-0.005601</td>\n",
       "      <td>-0.021510</td>\n",
       "      <td>0.012467</td>\n",
       "      <td>-0.126067</td>\n",
       "      <td>0.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sub-049</td>\n",
       "      <td>-0.010554</td>\n",
       "      <td>-0.003805</td>\n",
       "      <td>-0.001752</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>-0.013817</td>\n",
       "      <td>-0.009263</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>-0.000785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>-0.004856</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.010031</td>\n",
       "      <td>0.010606</td>\n",
       "      <td>-0.004154</td>\n",
       "      <td>0.012414</td>\n",
       "      <td>0.393250</td>\n",
       "      <td>0.002404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sub-050</td>\n",
       "      <td>-0.026579</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>-0.000297</td>\n",
       "      <td>0.009220</td>\n",
       "      <td>-0.031404</td>\n",
       "      <td>-0.000610</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001313</td>\n",
       "      <td>-0.004491</td>\n",
       "      <td>-0.010932</td>\n",
       "      <td>-0.007466</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.004987</td>\n",
       "      <td>-0.024018</td>\n",
       "      <td>0.012427</td>\n",
       "      <td>0.467136</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sub-051</td>\n",
       "      <td>-0.003496</td>\n",
       "      <td>-0.001667</td>\n",
       "      <td>0.006838</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>-0.005746</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.008626</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.899174</td>\n",
       "      <td>0.002395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sub-054</td>\n",
       "      <td>-0.007850</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>-0.006259</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>-0.001189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014845</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>-0.008649</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.012798</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>-0.006649</td>\n",
       "      <td>0.012434</td>\n",
       "      <td>0.160411</td>\n",
       "      <td>0.002399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sub-052</td>\n",
       "      <td>-0.020849</td>\n",
       "      <td>-0.017017</td>\n",
       "      <td>-0.017000</td>\n",
       "      <td>-0.020237</td>\n",
       "      <td>-0.024554</td>\n",
       "      <td>-0.027895</td>\n",
       "      <td>-0.014614</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>-0.036619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030557</td>\n",
       "      <td>-0.025966</td>\n",
       "      <td>-0.030551</td>\n",
       "      <td>-0.029962</td>\n",
       "      <td>-0.023455</td>\n",
       "      <td>-0.025625</td>\n",
       "      <td>-0.019577</td>\n",
       "      <td>0.012547</td>\n",
       "      <td>0.128530</td>\n",
       "      <td>0.002420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sub-053</td>\n",
       "      <td>-0.009593</td>\n",
       "      <td>-0.013785</td>\n",
       "      <td>-0.010336</td>\n",
       "      <td>-0.008232</td>\n",
       "      <td>-0.034691</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>-0.003743</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.008078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005989</td>\n",
       "      <td>-0.020166</td>\n",
       "      <td>-0.016371</td>\n",
       "      <td>-0.012259</td>\n",
       "      <td>-0.002227</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>-0.024349</td>\n",
       "      <td>0.012494</td>\n",
       "      <td>-0.049039</td>\n",
       "      <td>0.002410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sub-009</td>\n",
       "      <td>-0.018977</td>\n",
       "      <td>-0.012562</td>\n",
       "      <td>-0.003156</td>\n",
       "      <td>-0.029378</td>\n",
       "      <td>-0.005898</td>\n",
       "      <td>-0.010873</td>\n",
       "      <td>-0.000858</td>\n",
       "      <td>-0.017093</td>\n",
       "      <td>-0.027377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>-0.019951</td>\n",
       "      <td>-0.036140</td>\n",
       "      <td>-0.025300</td>\n",
       "      <td>-0.012703</td>\n",
       "      <td>-0.025295</td>\n",
       "      <td>-0.022501</td>\n",
       "      <td>0.012303</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.002838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sub-006</td>\n",
       "      <td>-0.005962</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.022881</td>\n",
       "      <td>0.012168</td>\n",
       "      <td>0.008540</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>-0.007154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013867</td>\n",
       "      <td>-0.003909</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>-0.009388</td>\n",
       "      <td>-0.007416</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.006602</td>\n",
       "      <td>0.012423</td>\n",
       "      <td>-0.186586</td>\n",
       "      <td>0.002769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sub-003</td>\n",
       "      <td>-0.026259</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.006117</td>\n",
       "      <td>-0.016435</td>\n",
       "      <td>-0.020814</td>\n",
       "      <td>-0.001995</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>-0.009655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.009827</td>\n",
       "      <td>-0.016021</td>\n",
       "      <td>-0.013534</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.023162</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>1.379198</td>\n",
       "      <td>0.002720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sub-004</td>\n",
       "      <td>-0.013378</td>\n",
       "      <td>-0.004397</td>\n",
       "      <td>-0.006210</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sub-011</td>\n",
       "      <td>-0.003784</td>\n",
       "      <td>-0.008590</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>-0.015636</td>\n",
       "      <td>-0.009475</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>-0.005100</td>\n",
       "      <td>-0.008298</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003037</td>\n",
       "      <td>-0.007321</td>\n",
       "      <td>-0.010164</td>\n",
       "      <td>-0.008139</td>\n",
       "      <td>-0.012895</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>-0.019547</td>\n",
       "      <td>0.012348</td>\n",
       "      <td>-0.158400</td>\n",
       "      <td>0.002822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sub-008</td>\n",
       "      <td>-0.004646</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>-0.006786</td>\n",
       "      <td>0.012283</td>\n",
       "      <td>0.008456</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-0.010536</td>\n",
       "      <td>-0.005950</td>\n",
       "      <td>0.006119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002970</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>0.016033</td>\n",
       "      <td>0.006506</td>\n",
       "      <td>0.010104</td>\n",
       "      <td>-0.005694</td>\n",
       "      <td>0.015446</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.075905</td>\n",
       "      <td>0.002843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sub-001</td>\n",
       "      <td>-0.006885</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>-0.013197</td>\n",
       "      <td>-0.013694</td>\n",
       "      <td>-0.009282</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>-0.010254</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.013844</td>\n",
       "      <td>-0.011792</td>\n",
       "      <td>-0.009998</td>\n",
       "      <td>-0.012524</td>\n",
       "      <td>-0.025626</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>1.830811</td>\n",
       "      <td>0.002748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>0.013835</td>\n",
       "      <td>0.015602</td>\n",
       "      <td>0.006617</td>\n",
       "      <td>-0.002029</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.008777</td>\n",
       "      <td>0.017131</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009466</td>\n",
       "      <td>-0.003701</td>\n",
       "      <td>0.011509</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.009692</td>\n",
       "      <td>-0.008439</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>2.803576</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.002710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sub-005</td>\n",
       "      <td>-0.030890</td>\n",
       "      <td>0.016271</td>\n",
       "      <td>-0.006028</td>\n",
       "      <td>-0.010811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sub-007</td>\n",
       "      <td>-0.010039</td>\n",
       "      <td>-0.001393</td>\n",
       "      <td>-0.003494</td>\n",
       "      <td>-0.005115</td>\n",
       "      <td>-0.010285</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>-0.001367</td>\n",
       "      <td>0.006417</td>\n",
       "      <td>0.018901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.011959</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.012321</td>\n",
       "      <td>0.379675</td>\n",
       "      <td>1.040810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sub-013</td>\n",
       "      <td>-0.007604</td>\n",
       "      <td>0.021150</td>\n",
       "      <td>0.026183</td>\n",
       "      <td>-0.010864</td>\n",
       "      <td>-0.001817</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>0.014757</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.017350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>-0.003509</td>\n",
       "      <td>-0.010686</td>\n",
       "      <td>0.021952</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>0.017762</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.158007</td>\n",
       "      <td>0.002780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sub-014</td>\n",
       "      <td>-0.021166</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>-0.025047</td>\n",
       "      <td>-0.020685</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>-0.002398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011067</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>-0.010662</td>\n",
       "      <td>-0.016261</td>\n",
       "      <td>-0.000505</td>\n",
       "      <td>-0.005579</td>\n",
       "      <td>-0.012554</td>\n",
       "      <td>0.012380</td>\n",
       "      <td>0.144423</td>\n",
       "      <td>0.002786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sub-010</td>\n",
       "      <td>-0.003868</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>0.012339</td>\n",
       "      <td>-0.006504</td>\n",
       "      <td>-0.029840</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>-0.002428</td>\n",
       "      <td>0.016153</td>\n",
       "      <td>-0.003344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002402</td>\n",
       "      <td>-0.006424</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>-0.000462</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>-0.005028</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.002816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sub-012</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>-0.013619</td>\n",
       "      <td>0.004430</td>\n",
       "      <td>-0.008633</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.007949</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004151</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.008780</td>\n",
       "      <td>-0.019290</td>\n",
       "      <td>-0.005178</td>\n",
       "      <td>-0.000259</td>\n",
       "      <td>-0.010479</td>\n",
       "      <td>0.012151</td>\n",
       "      <td>1.786867</td>\n",
       "      <td>0.002802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sub-019</td>\n",
       "      <td>-0.023188</td>\n",
       "      <td>-0.006138</td>\n",
       "      <td>-0.024324</td>\n",
       "      <td>-0.030981</td>\n",
       "      <td>-0.004326</td>\n",
       "      <td>-0.016813</td>\n",
       "      <td>-0.013119</td>\n",
       "      <td>-0.018880</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>-0.015837</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>-0.014997</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>-0.020649</td>\n",
       "      <td>-0.022581</td>\n",
       "      <td>0.012412</td>\n",
       "      <td>-1.572366</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sub-017</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.015359</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.018864</td>\n",
       "      <td>0.017139</td>\n",
       "      <td>0.013565</td>\n",
       "      <td>-0.008571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015830</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>-0.007468</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>0.010798</td>\n",
       "      <td>0.016470</td>\n",
       "      <td>-0.004531</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>-2.197445</td>\n",
       "      <td>0.002845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>sub-023</td>\n",
       "      <td>-0.018075</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>-0.008042</td>\n",
       "      <td>-0.008847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sub-030</td>\n",
       "      <td>-0.014556</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>-0.011779</td>\n",
       "      <td>-0.015137</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>-0.003198</td>\n",
       "      <td>-0.005661</td>\n",
       "      <td>-0.009769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.003940</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>-0.012381</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>-0.014482</td>\n",
       "      <td>0.012458</td>\n",
       "      <td>-0.428367</td>\n",
       "      <td>0.002377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>sub-024</td>\n",
       "      <td>-0.009369</td>\n",
       "      <td>0.013676</td>\n",
       "      <td>-0.001488</td>\n",
       "      <td>-0.006310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>sub-020</td>\n",
       "      <td>-0.018326</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>-0.015240</td>\n",
       "      <td>-0.020218</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>-0.014510</td>\n",
       "      <td>-0.010187</td>\n",
       "      <td>-0.010741</td>\n",
       "      <td>-0.016095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.005115</td>\n",
       "      <td>-0.018906</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.011755</td>\n",
       "      <td>-0.006386</td>\n",
       "      <td>-0.015462</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>1.342206</td>\n",
       "      <td>0.002794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sub-015</td>\n",
       "      <td>-0.004214</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>-0.007950</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>-0.002050</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>0.005877</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.011908</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.006592</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>-0.412652</td>\n",
       "      <td>0.002786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sub-016</td>\n",
       "      <td>-0.022919</td>\n",
       "      <td>-0.006373</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>-0.021038</td>\n",
       "      <td>-0.018714</td>\n",
       "      <td>-0.008452</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>-0.009915</td>\n",
       "      <td>-0.013902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005259</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>-0.019134</td>\n",
       "      <td>-0.012623</td>\n",
       "      <td>-0.004652</td>\n",
       "      <td>-0.024876</td>\n",
       "      <td>-0.018163</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>-0.072109</td>\n",
       "      <td>0.002807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>sub-018</td>\n",
       "      <td>-0.016266</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>-0.012825</td>\n",
       "      <td>-0.019512</td>\n",
       "      <td>-0.009050</td>\n",
       "      <td>0.007293</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>-0.003163</td>\n",
       "      <td>-0.013140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.007967</td>\n",
       "      <td>-0.020328</td>\n",
       "      <td>-0.009820</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>-0.003045</td>\n",
       "      <td>-0.015260</td>\n",
       "      <td>0.012659</td>\n",
       "      <td>0.712672</td>\n",
       "      <td>0.002788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sub-027</td>\n",
       "      <td>-0.025314</td>\n",
       "      <td>-0.010804</td>\n",
       "      <td>-0.015215</td>\n",
       "      <td>-0.015543</td>\n",
       "      <td>-0.004314</td>\n",
       "      <td>-0.017146</td>\n",
       "      <td>-0.011580</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>-0.016228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011510</td>\n",
       "      <td>-0.016077</td>\n",
       "      <td>-0.017274</td>\n",
       "      <td>-0.009732</td>\n",
       "      <td>-0.013451</td>\n",
       "      <td>-0.015584</td>\n",
       "      <td>-0.003202</td>\n",
       "      <td>0.012476</td>\n",
       "      <td>-2.475861</td>\n",
       "      <td>0.002408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>sub-029</td>\n",
       "      <td>-0.031649</td>\n",
       "      <td>-0.005814</td>\n",
       "      <td>-0.005986</td>\n",
       "      <td>-0.033177</td>\n",
       "      <td>-0.014575</td>\n",
       "      <td>-0.010381</td>\n",
       "      <td>-0.012394</td>\n",
       "      <td>-0.002355</td>\n",
       "      <td>-0.010838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016902</td>\n",
       "      <td>-0.019219</td>\n",
       "      <td>-0.023572</td>\n",
       "      <td>-0.003780</td>\n",
       "      <td>-0.003424</td>\n",
       "      <td>-0.008452</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.083738</td>\n",
       "      <td>0.127174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>sub-021</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>-0.007987</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>sub-026</td>\n",
       "      <td>-0.020392</td>\n",
       "      <td>-0.017453</td>\n",
       "      <td>-0.011094</td>\n",
       "      <td>-0.008317</td>\n",
       "      <td>-0.018687</td>\n",
       "      <td>-0.036872</td>\n",
       "      <td>-0.020923</td>\n",
       "      <td>-0.010768</td>\n",
       "      <td>-0.010471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009185</td>\n",
       "      <td>-0.007560</td>\n",
       "      <td>-0.015619</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>-0.025524</td>\n",
       "      <td>-0.018997</td>\n",
       "      <td>-0.014005</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>-0.048120</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>sub-025</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.015508</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>-0.005478</td>\n",
       "      <td>-0.000978</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.024699</td>\n",
       "      <td>-0.010843</td>\n",
       "      <td>-0.004979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012893</td>\n",
       "      <td>-0.000395</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>-0.001652</td>\n",
       "      <td>0.013306</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>1.291765</td>\n",
       "      <td>0.002810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>sub-022</td>\n",
       "      <td>-0.014094</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>-0.010101</td>\n",
       "      <td>-0.015767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>sub-028</td>\n",
       "      <td>-0.009472</td>\n",
       "      <td>-0.021044</td>\n",
       "      <td>-0.005924</td>\n",
       "      <td>-0.003041</td>\n",
       "      <td>-0.006396</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>-0.013403</td>\n",
       "      <td>-0.009269</td>\n",
       "      <td>-0.009718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001085</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>-0.005296</td>\n",
       "      <td>-0.001424</td>\n",
       "      <td>-0.001071</td>\n",
       "      <td>-0.006037</td>\n",
       "      <td>-0.032551</td>\n",
       "      <td>0.012435</td>\n",
       "      <td>0.904518</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>sub-035</td>\n",
       "      <td>-0.028024</td>\n",
       "      <td>-0.021713</td>\n",
       "      <td>-0.004728</td>\n",
       "      <td>-0.036846</td>\n",
       "      <td>-0.034788</td>\n",
       "      <td>-0.033512</td>\n",
       "      <td>-0.023694</td>\n",
       "      <td>-0.029843</td>\n",
       "      <td>-0.038047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008879</td>\n",
       "      <td>-0.023030</td>\n",
       "      <td>-0.035977</td>\n",
       "      <td>-0.018398</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>-0.032982</td>\n",
       "      <td>-0.035959</td>\n",
       "      <td>0.012639</td>\n",
       "      <td>0.249352</td>\n",
       "      <td>0.002434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>sub-031</td>\n",
       "      <td>-0.009776</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>-0.007577</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>-0.013882</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.013033</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>-0.011167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.009339</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>-0.005748</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.012343</td>\n",
       "      <td>-0.233452</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>sub-032</td>\n",
       "      <td>-0.001825</td>\n",
       "      <td>-0.005650</td>\n",
       "      <td>-0.007588</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>-0.014380</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>-0.001432</td>\n",
       "      <td>-0.008549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009978</td>\n",
       "      <td>-0.010332</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>-0.022208</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>-0.303437</td>\n",
       "      <td>0.002412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>sub-034</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>0.016240</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.019051</td>\n",
       "      <td>-0.004328</td>\n",
       "      <td>0.016132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.013647</td>\n",
       "      <td>0.010182</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.007386</td>\n",
       "      <td>0.013017</td>\n",
       "      <td>0.012685</td>\n",
       "      <td>-1.553555</td>\n",
       "      <td>0.002401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>sub-033</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.009194</td>\n",
       "      <td>-0.020096</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>0.017047</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008385</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.011454</td>\n",
       "      <td>0.009648</td>\n",
       "      <td>0.012178</td>\n",
       "      <td>0.012378</td>\n",
       "      <td>0.007105</td>\n",
       "      <td>0.012503</td>\n",
       "      <td>0.176115</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54 rows  67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id       fp1        fz        f3        f7  below_eye  \\\n",
       "0         sub-040 -0.003777  0.001629  0.004618  0.007591  -0.012560   \n",
       "1         sub-038 -0.033660 -0.001765 -0.012161 -0.019336  -0.037943   \n",
       "2         sub-042 -0.022187 -0.018431 -0.017252 -0.017319  -0.014809   \n",
       "3         sub-046  0.016337  0.007892 -0.005101  0.004846   0.003703   \n",
       "4         sub-043 -0.007540  0.011554 -0.003990 -0.001310  -0.007328   \n",
       "5         sub-041 -0.008371  0.000245 -0.009281 -0.003000  -0.015570   \n",
       "6         sub-036  0.001529  0.001408 -0.005508 -0.012362  -0.011743   \n",
       "7         sub-037 -0.022464 -0.022063 -0.017743 -0.027273  -0.018002   \n",
       "8         sub-039  0.010320  0.025463  0.011351  0.028762   0.012944   \n",
       "9         sub-045  0.020870  0.023257  0.013297  0.026966   0.004097   \n",
       "10        sub-047 -0.011584 -0.000613 -0.005707 -0.002414  -0.018764   \n",
       "11        sub-044 -0.005708 -0.003644  0.003865 -0.001842  -0.017365   \n",
       "12        sub-048 -0.006969 -0.010642 -0.012793 -0.025560  -0.015865   \n",
       "13        sub-049 -0.010554 -0.003805 -0.001752  0.007886  -0.013817   \n",
       "14        sub-050 -0.026579  0.001780 -0.000297  0.009220  -0.031404   \n",
       "15        sub-051 -0.003496 -0.001667  0.006838  0.009129  -0.005746   \n",
       "16        sub-054 -0.007850  0.007753  0.003175  0.010630  -0.006259   \n",
       "17        sub-052 -0.020849 -0.017017 -0.017000 -0.020237  -0.024554   \n",
       "18        sub-053 -0.009593 -0.013785 -0.010336 -0.008232  -0.034691   \n",
       "19        sub-009 -0.018977 -0.012562 -0.003156 -0.029378  -0.005898   \n",
       "20        sub-006 -0.005962  0.014713  0.005335  0.022881   0.012168   \n",
       "21        sub-003 -0.026259 -0.000343 -0.000536 -0.006117  -0.016435   \n",
       "22        sub-004 -0.013378 -0.004397 -0.006210  0.003812        NaN   \n",
       "23        sub-011 -0.003784 -0.008590 -0.007245 -0.015636  -0.009475   \n",
       "24        sub-008 -0.004646  0.002468 -0.006786  0.012283   0.008456   \n",
       "25        sub-001 -0.006885  0.001531 -0.002528 -0.013197  -0.013694   \n",
       "26        sub-002  0.006728  0.013835  0.015602  0.006617  -0.002029   \n",
       "27        sub-005 -0.030890  0.016271 -0.006028 -0.010811        NaN   \n",
       "28        sub-007 -0.010039 -0.001393 -0.003494 -0.005115  -0.010285   \n",
       "29        sub-013 -0.007604  0.021150  0.026183 -0.010864  -0.001817   \n",
       "30        sub-014 -0.021166  0.000999  0.000526 -0.025047  -0.020685   \n",
       "31        sub-010 -0.003868 -0.002087  0.012339 -0.006504  -0.029840   \n",
       "32        sub-012  0.005726  0.001772 -0.001047 -0.013619   0.004430   \n",
       "33        sub-019 -0.023188 -0.006138 -0.024324 -0.030981  -0.004326   \n",
       "34        sub-017  0.003264  0.006245  0.015359  0.014401   0.001798   \n",
       "35        sub-023 -0.018075  0.000770 -0.008042 -0.008847        NaN   \n",
       "36        sub-030 -0.014556  0.001812 -0.000285 -0.011779  -0.015137   \n",
       "37        sub-024 -0.009369  0.013676 -0.001488 -0.006310        NaN   \n",
       "38        sub-020 -0.018326  0.000844 -0.015240 -0.020218  -0.001486   \n",
       "39        sub-015 -0.004214  0.001439 -0.007950  0.001258  -0.002050   \n",
       "40        sub-016 -0.022919 -0.006373  0.001190 -0.021038  -0.018714   \n",
       "41        sub-018 -0.016266  0.001283 -0.012825 -0.019512  -0.009050   \n",
       "42        sub-027 -0.025314 -0.010804 -0.015215 -0.015543  -0.004314   \n",
       "43        sub-029 -0.031649 -0.005814 -0.005986 -0.033177  -0.014575   \n",
       "44        sub-021  0.002524  0.003137 -0.007987  0.000470        NaN   \n",
       "45        sub-026 -0.020392 -0.017453 -0.011094 -0.008317  -0.018687   \n",
       "46        sub-025  0.001850  0.015508  0.009505 -0.005478  -0.000978   \n",
       "47        sub-022 -0.014094  0.002646 -0.010101 -0.015767        NaN   \n",
       "48        sub-028 -0.009472 -0.021044 -0.005924 -0.003041  -0.006396   \n",
       "49        sub-035 -0.028024 -0.021713 -0.004728 -0.036846  -0.034788   \n",
       "50        sub-031 -0.009776  0.001448 -0.007577  0.004197  -0.013882   \n",
       "51        sub-032 -0.001825 -0.005650 -0.007588  0.000786  -0.014380   \n",
       "52        sub-034  0.002890  0.000224  0.003169  0.016240   0.005839   \n",
       "53        sub-033  0.003262  0.013233  0.001616  0.009194  -0.020096   \n",
       "\n",
       "         fc5       fc1        c3        t7  ...        c2       fc4       ft8  \\\n",
       "0   0.000633  0.008017  0.002405  0.006219  ...  0.000610  0.004524 -0.014736   \n",
       "1   0.000951 -0.008832  0.000273 -0.005543  ... -0.007977 -0.016760 -0.015461   \n",
       "2  -0.001071 -0.009401 -0.000777 -0.000968  ...  0.000226 -0.009453 -0.011647   \n",
       "3   0.005604 -0.004049  0.002857 -0.004978  ...  0.005497  0.021531  0.010498   \n",
       "4  -0.000084 -0.000146  0.004025 -0.000780  ...  0.004153  0.006771  0.002368   \n",
       "5   0.001018 -0.003218  0.007129 -0.014805  ...  0.006650 -0.002724 -0.008276   \n",
       "6  -0.003125 -0.002425 -0.002463  0.003823  ... -0.001891 -0.012248 -0.019721   \n",
       "7  -0.010503 -0.015123 -0.007808 -0.015364  ...  0.001114 -0.016360 -0.015877   \n",
       "8   0.000745  0.026632  0.020138  0.010270  ...  0.010249  0.015135  0.007428   \n",
       "9   0.029131  0.019025  0.018251  0.031693  ...  0.028374  0.011032 -0.005392   \n",
       "10  0.000951  0.000630  0.010615  0.000799  ...  0.002686 -0.002309  0.002051   \n",
       "11 -0.005644 -0.009976 -0.006106 -0.003016  ... -0.007209 -0.009449 -0.000202   \n",
       "12 -0.026004 -0.021883 -0.007552 -0.024513  ... -0.006648 -0.006894 -0.017713   \n",
       "13 -0.009263  0.007793  0.009009 -0.000785  ...  0.009732  0.004497 -0.004856   \n",
       "14 -0.000610  0.006108  0.004235  0.007902  ... -0.001313 -0.004491 -0.010932   \n",
       "15  0.003008  0.004419  0.002398  0.002503  ...  0.002133  0.008626  0.005012   \n",
       "16  0.001018  0.008746  0.013203 -0.001189  ...  0.014845  0.006607 -0.008649   \n",
       "17 -0.027895 -0.014614 -0.009901 -0.036619  ... -0.030557 -0.025966 -0.030551   \n",
       "18  0.002812 -0.003743 -0.026088 -0.008078  ... -0.005989 -0.020166 -0.016371   \n",
       "19 -0.010873 -0.000858 -0.017093 -0.027377  ...  0.004662 -0.019951 -0.036140   \n",
       "20  0.008540  0.000893  0.004453 -0.007154  ...  0.013867 -0.003909  0.000481   \n",
       "21 -0.020814 -0.001995  0.000428 -0.009655  ... -0.001402 -0.009827 -0.016021   \n",
       "22       NaN       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "23  0.003413 -0.005100 -0.008298  0.008853  ... -0.003037 -0.007321 -0.010164   \n",
       "24 -0.000412 -0.010536 -0.005950  0.006119  ... -0.002970  0.005766  0.016033   \n",
       "25 -0.009282  0.004364 -0.010254 -0.000251  ... -0.006757 -0.000072 -0.013844   \n",
       "26  0.013220  0.008777  0.017131  0.010959  ... -0.009466 -0.003701  0.011509   \n",
       "27       NaN       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "28  0.004411 -0.001367  0.006417  0.018901  ...  0.009685  0.008228  0.011959   \n",
       "29  0.006965  0.014757  0.009901  0.017350  ...  0.015193 -0.003509 -0.010686   \n",
       "30  0.000332  0.005081  0.000796 -0.002398  ...  0.011067 -0.001871 -0.010662   \n",
       "31  0.004096 -0.002428  0.016153 -0.003344  ... -0.002402 -0.006424 -0.008929   \n",
       "32 -0.008633  0.001295 -0.007949  0.001361  ... -0.004151  0.001691  0.008780   \n",
       "33 -0.016813 -0.013119 -0.018880 -0.008934  ...  0.002857 -0.015837  0.002011   \n",
       "34  0.018864  0.017139  0.013565 -0.008571  ...  0.015830  0.007040 -0.007468   \n",
       "35       NaN       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "36  0.005488 -0.003198 -0.005661 -0.009769  ...  0.001877 -0.009227 -0.003940   \n",
       "37       NaN       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "38 -0.014510 -0.010187 -0.010741 -0.016095  ...  0.000032 -0.005115 -0.018906   \n",
       "39  0.010460  0.005877  0.002674  0.000911  ...  0.001625  0.003470 -0.010042   \n",
       "40 -0.008452 -0.000578 -0.009915 -0.013902  ... -0.005259 -0.001397 -0.019134   \n",
       "41  0.007293  0.003147 -0.003163 -0.013140  ...  0.000712 -0.007967 -0.020328   \n",
       "42 -0.017146 -0.011580 -0.012376 -0.016228  ... -0.011510 -0.016077 -0.017274   \n",
       "43 -0.010381 -0.012394 -0.002355 -0.010838  ... -0.016902 -0.019219 -0.023572   \n",
       "44       NaN       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "45 -0.036872 -0.020923 -0.010768 -0.010471  ... -0.009185 -0.007560 -0.015619   \n",
       "46  0.009650  0.024699 -0.010843 -0.004979  ...  0.012893 -0.000395  0.004700   \n",
       "47       NaN       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "48  0.002567 -0.013403 -0.009269 -0.009718  ... -0.001085  0.001115 -0.005296   \n",
       "49 -0.033512 -0.023694 -0.029843 -0.038047  ... -0.008879 -0.023030 -0.035977   \n",
       "50  0.002369  0.013033  0.006593 -0.011167  ... -0.000199 -0.009339  0.005535   \n",
       "51  0.001227  0.003904 -0.001432 -0.008549  ... -0.009978 -0.010332  0.001560   \n",
       "52  0.003258  0.019051 -0.004328  0.016132  ...  0.008072  0.013647  0.010182   \n",
       "53  0.012927  0.017047  0.015873  0.007542  ...  0.008385  0.001924  0.011454   \n",
       "\n",
       "          f6        f2       af4       af8     empty       ekg  audiooutput  \n",
       "0   0.003725  0.001829 -0.002208 -0.000135  0.012523 -1.346364     0.002399  \n",
       "1  -0.003301 -0.005819 -0.019429 -0.017650  0.012533 -0.015123     0.002376  \n",
       "2  -0.010740 -0.005030 -0.006930 -0.004330  0.012517 -0.447032     0.002381  \n",
       "3  -0.000772  0.024353  0.000548 -0.003559  0.012418 -0.716166     0.002380  \n",
       "4   0.007011  0.002105 -0.004701 -0.013563  0.012420 -1.083249     0.002370  \n",
       "5   0.005639  0.005128 -0.008720 -0.021106  0.012515 -0.490343     0.002397  \n",
       "6   0.000248 -0.007371 -0.003281 -0.012928  0.012365  0.728034     0.002414  \n",
       "7  -0.010257 -0.019731 -0.017743 -0.019444  0.012538  0.078153     0.002373  \n",
       "8   0.020085  0.010579  0.029482  0.007482  0.012496 -0.207660     0.002457  \n",
       "9   0.023527  0.023697  0.015078  0.000156  0.012270 -0.413792     0.002360  \n",
       "10  0.000994  0.001647 -0.003719 -0.023669  0.012729  0.015950     0.002386  \n",
       "11 -0.010831  0.001413 -0.017923 -0.008670  0.012566  0.065308     0.002373  \n",
       "12 -0.017027 -0.005503 -0.005601 -0.021510  0.012467 -0.126067     0.002388  \n",
       "13  0.005499  0.010031  0.010606 -0.004154  0.012414  0.393250     0.002404  \n",
       "14 -0.007466  0.002230  0.004987 -0.024018  0.012427  0.467136     0.002394  \n",
       "15  0.003898  0.006573  0.001223 -0.010261  0.012200  0.899174     0.002395  \n",
       "16  0.014261  0.012798 -0.002769 -0.006649  0.012434  0.160411     0.002399  \n",
       "17 -0.029962 -0.023455 -0.025625 -0.019577  0.012547  0.128530     0.002420  \n",
       "18 -0.012259 -0.002227  0.001944 -0.024349  0.012494 -0.049039     0.002410  \n",
       "19 -0.025300 -0.012703 -0.025295 -0.022501  0.012303  0.050009     0.002838  \n",
       "20 -0.009388 -0.007416  0.000051 -0.006602  0.012423 -0.186586     0.002769  \n",
       "21 -0.013534  0.005141  0.000050 -0.023162  0.012644  1.379198     0.002720  \n",
       "22       NaN       NaN       NaN       NaN       NaN       NaN          NaN  \n",
       "23 -0.008139 -0.012895  0.002079 -0.019547  0.012348 -0.158400     0.002822  \n",
       "24  0.006506  0.010104 -0.005694  0.015446  0.012235  0.075905     0.002843  \n",
       "25 -0.011792 -0.009998 -0.012524 -0.025626  0.012368  1.830811     0.002748  \n",
       "26  0.003604  0.009692 -0.008439  0.004798  2.803576  0.004948     0.002710  \n",
       "27       NaN       NaN       NaN       NaN       NaN       NaN          NaN  \n",
       "28  0.005457  0.002437  0.007888  0.001032  0.012321  0.379675     1.040810  \n",
       "29  0.021952  0.011366  0.017762  0.005693  0.012167  0.158007     0.002780  \n",
       "30 -0.016261 -0.000505 -0.005579 -0.012554  0.012380  0.144423     0.002786  \n",
       "31 -0.000462 -0.003708 -0.001394 -0.005028  0.012398  0.233600     0.002816  \n",
       "32 -0.019290 -0.005178 -0.000259 -0.010479  0.012151  1.786867     0.002802  \n",
       "33 -0.014997  0.001220 -0.020649 -0.022581  0.012412 -1.572366     0.002800  \n",
       "34  0.003229  0.010798  0.016470 -0.004531  0.012424 -2.197445     0.002845  \n",
       "35       NaN       NaN       NaN       NaN       NaN       NaN          NaN  \n",
       "36  0.005818 -0.012381  0.006570 -0.014482  0.012458 -0.428367     0.002377  \n",
       "37       NaN       NaN       NaN       NaN       NaN       NaN          NaN  \n",
       "38 -0.006818 -0.011755 -0.006386 -0.015462  0.012292  1.342206     0.002794  \n",
       "39 -0.011908  0.005528  0.000147 -0.006592  0.012275 -0.412652     0.002786  \n",
       "40 -0.012623 -0.004652 -0.024876 -0.018163  0.012411 -0.072109     0.002807  \n",
       "41 -0.009820 -0.005167 -0.003045 -0.015260  0.012659  0.712672     0.002788  \n",
       "42 -0.009732 -0.013451 -0.015584 -0.003202  0.012476 -2.475861     0.002408  \n",
       "43 -0.003780 -0.003424 -0.008452 -0.009568  0.012376  0.083738     0.127174  \n",
       "44       NaN       NaN       NaN       NaN       NaN       NaN          NaN  \n",
       "45 -0.002317 -0.025524 -0.018997 -0.014005  0.012448 -0.048120     0.002853  \n",
       "46  0.003756  0.016368 -0.001652  0.013306  0.012359  1.291765     0.002810  \n",
       "47       NaN       NaN       NaN       NaN       NaN       NaN          NaN  \n",
       "48 -0.001424 -0.001071 -0.006037 -0.032551  0.012435  0.904518     0.002394  \n",
       "49 -0.018398  0.000582 -0.032982 -0.035959  0.012639  0.249352     0.002434  \n",
       "50  0.002731 -0.005748  0.001734  0.010492  0.012343 -0.233452     0.002394  \n",
       "51 -0.019524  0.002383  0.003539 -0.022208  0.012585 -0.303437     0.002412  \n",
       "52  0.004485  0.002686  0.007386  0.013017  0.012685 -1.553555     0.002401  \n",
       "53  0.009648  0.012178  0.012378  0.007105  0.012503  0.176115     0.002394  \n",
       "\n",
       "[54 rows x 67 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDataSQl = pd.read_sql_query(\"SELECT * FROM eeg_data\", conn)\n",
    "\n",
    "rawDataSQl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeDF = pd.merge(participant, rawDataSQl, on='participant_id')\n",
    "\n",
    "mergeDF.to_csv(r'/Users/natchapon/Documents/data_sci/merge_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_state = pd.read_csv(r'/Users/natchapon/Documents/data_sci/merge_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>Original_ID</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>BDI</th>\n",
       "      <th>AUDIT</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>fp1</th>\n",
       "      <th>fz</th>\n",
       "      <th>...</th>\n",
       "      <th>c2</th>\n",
       "      <th>fc4</th>\n",
       "      <th>ft8</th>\n",
       "      <th>f6</th>\n",
       "      <th>f2</th>\n",
       "      <th>af4</th>\n",
       "      <th>af8</th>\n",
       "      <th>empty</th>\n",
       "      <th>ekg</th>\n",
       "      <th>audiooutput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>sub-036</td>\n",
       "      <td>10637</td>\n",
       "      <td>F</td>\n",
       "      <td>31</td>\n",
       "      <td>CTL</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>-0.012248</td>\n",
       "      <td>-0.019721</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>-0.007371</td>\n",
       "      <td>-0.003281</td>\n",
       "      <td>-0.012928</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>0.728034</td>\n",
       "      <td>0.002414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-003</td>\n",
       "      <td>10602</td>\n",
       "      <td>F</td>\n",
       "      <td>22</td>\n",
       "      <td>CTL</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.026259</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.009827</td>\n",
       "      <td>-0.016021</td>\n",
       "      <td>-0.013534</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.023162</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>1.379198</td>\n",
       "      <td>0.002720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sub-030</td>\n",
       "      <td>10631</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>ALC</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.014556</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.003940</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>-0.012381</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>-0.014482</td>\n",
       "      <td>0.012458</td>\n",
       "      <td>-0.428367</td>\n",
       "      <td>0.002377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-001</td>\n",
       "      <td>10600</td>\n",
       "      <td>F</td>\n",
       "      <td>43</td>\n",
       "      <td>CTL</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.006885</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.013844</td>\n",
       "      <td>-0.011792</td>\n",
       "      <td>-0.009998</td>\n",
       "      <td>-0.012524</td>\n",
       "      <td>-0.025626</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>1.830811</td>\n",
       "      <td>0.002748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub-010</td>\n",
       "      <td>10609</td>\n",
       "      <td>F</td>\n",
       "      <td>54</td>\n",
       "      <td>ALC</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.003868</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002402</td>\n",
       "      <td>-0.006424</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>-0.000462</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>-0.005028</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.002816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id  Original_ID sex  age GROUP  BDI  AUDIT  EDUCATION  \\\n",
       "35        sub-036        10637   F   31   CTL    4      1         22   \n",
       "2         sub-003        10602   F   22   CTL    2      1         18   \n",
       "29        sub-030        10631   F   45   ALC    8      7         17   \n",
       "0         sub-001        10600   F   43   CTL   14      1         16   \n",
       "9         sub-010        10609   F   54   ALC   17      9         12   \n",
       "\n",
       "         fp1        fz  ...        c2       fc4       ft8        f6        f2  \\\n",
       "35  0.001529  0.001408  ... -0.001891 -0.012248 -0.019721  0.000248 -0.007371   \n",
       "2  -0.026259 -0.000343  ... -0.001402 -0.009827 -0.016021 -0.013534  0.005141   \n",
       "29 -0.014556  0.001812  ...  0.001877 -0.009227 -0.003940  0.005818 -0.012381   \n",
       "0  -0.006885  0.001531  ... -0.006757 -0.000072 -0.013844 -0.011792 -0.009998   \n",
       "9  -0.003868 -0.002087  ... -0.002402 -0.006424 -0.008929 -0.000462 -0.003708   \n",
       "\n",
       "         af4       af8     empty       ekg  audiooutput  \n",
       "35 -0.003281 -0.012928  0.012365  0.728034     0.002414  \n",
       "2   0.000050 -0.023162  0.012644  1.379198     0.002720  \n",
       "29  0.006570 -0.014482  0.012458 -0.428367     0.002377  \n",
       "0  -0.012524 -0.025626  0.012368  1.830811     0.002748  \n",
       "9  -0.001394 -0.005028  0.012398  0.233600     0.002816  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_state.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_state = eeg_state.drop(columns=['participant_id','Original_ID','sex','age','EDUCATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='count'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eeg_state['GROUP'].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_per_rows = eeg_state.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nan_per_rows.value_counts().plot(kind='pie')\n",
    "\n",
    "nan_mean_values = int(nan_per_rows.value_counts().mean())\n",
    "print(nan_mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def custom_classification_report(y_true, y_pred, target_names, class_indices):\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Initialize lists to store precision, recall, F1-score, and support for each class\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    support = []\n",
    "\n",
    "    # Calculate precision, recall, F1-score for each class\n",
    "    for i in range(len(target_names)):\n",
    "        true_positives = cm[i, i]\n",
    "        false_positives = cm[:, i].sum() - true_positives\n",
    "        false_negatives = cm[i, :].sum() - true_positives\n",
    "        true_negatives = cm.sum() - (true_positives + false_positives + false_negatives)\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        if true_positives + false_positives > 0:\n",
    "            precision_i = true_positives / (true_positives + false_positives)\n",
    "        else:\n",
    "            precision_i = 0.0\n",
    "\n",
    "        # Recall: TP / (TP + FN)\n",
    "        if true_positives + false_negatives > 0:\n",
    "            recall_i = true_positives / (true_positives + false_negatives)\n",
    "        else:\n",
    "            recall_i = 0.0\n",
    "\n",
    "        # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        if precision_i + recall_i > 0:\n",
    "            f1_i = 2 * (precision_i * recall_i) / (precision_i + recall_i)\n",
    "        else:\n",
    "            f1_i = 0.0\n",
    "\n",
    "        # Support: The number of true instances of each class\n",
    "        support_i = cm[i, :].sum()\n",
    "\n",
    "        # Append calculated metrics for this class\n",
    "        precision.append(precision_i)\n",
    "        recall.append(recall_i)\n",
    "        f1_score.append(f1_i)\n",
    "        support.append(support_i)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "    # Calculate average F1-score for specified classes\n",
    "    f1_average = np.mean([f1_score[i] for i in class_indices])\n",
    "\n",
    "    # Print the aesthetically improved report\n",
    "    print(\"\\n\" + \"Classification Report\".center(65, \"=\"))\n",
    "    print(f\"{'Class':<15}{'Precision':>12}{'Recall':>12}{'F1-Score':>12}{'Support':>12}\")\n",
    "    print(\"=\" * 65)\n",
    "    for i, label in enumerate(target_names):\n",
    "        print(f\"{label:<15}{precision[i]:>12.4f}{recall[i]:>12.4f}{f1_score[i]:>12.4f}{support[i]:>12}\")\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"Average F1-Score for classes : {f1_average:.4f}\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "class_indices = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/krwkffws1550dmpyl_kr36zw0000gn/T/ipykernel_47108/1929397760.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eeg_state_cleaned['GROUP'] = le.fit_transform(eeg_state_cleaned['GROUP'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# drop rows with missing values over nan_mean_values\n",
    "eeg_state_cleaned = eeg_state.dropna(thresh=eeg_state.shape[1] - nan_mean_values)\n",
    "# eeg_state_cleaned = eeg_data.dropna(thresh=eeg_data.shape[1]-11)\n",
    "\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "eeg_state_cleaned['GROUP'] = le.fit_transform(eeg_state_cleaned['GROUP'])\n",
    "\n",
    "# Convert object columns to numeric\n",
    "for col in eeg_state_cleaned.columns:\n",
    "    if eeg_state_cleaned[col].dtype == 'object':\n",
    "        eeg_state_cleaned[col] = pd.to_numeric(eeg_state_cleaned[col], errors='coerce')\n",
    "\n",
    "# Features and target\n",
    "X = eeg_state_cleaned.drop('GROUP', axis=1)\n",
    "y = eeg_state_cleaned['GROUP']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 2,  # Number of classes\n",
    "    'learning_rate': 0.2087759602169422,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.6478457734751482,\n",
    "    'colsample_bytree': 0.9431214021788126,\n",
    "    # 'device': 'cuda',  # Use GPU\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "train_data = xgb.DMatrix(X_train, label=y_train)\n",
    "test_data = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "xgb_model = xgb.train(params, train_data,num_boost_round=462)\n",
    "\n",
    "# Predict the class labels\n",
    "y_pred = xgb_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected features: ['BDI', 'AUDIT', 'fp1', 'fz', 'f3', 'f7', 'below_eye', 'fc5', 'fc1', 'c3', 't7', 'left_mastoid', 'cp5', 'cp1', 'pz', 'p3', 'p7', 'o1', 'oz', 'o2', 'p4', 'p8', 'right_mastoid', 'cp6', 'cp2', 'cz', 'c4', 't8', 'above_eye', 'fc6', 'fc2', 'f4', 'f8', 'fp2', 'af7', 'af3', 'afz', 'f1', 'f5', 'ft7', 'fc3', 'fcz', 'c1', 'c5', 'tp7', 'cp3', 'p1', 'p5', 'po7', 'po3', 'poz', 'po4', 'po8', 'p6', 'p2', 'cp4', 'tp8', 'c6', 'c2', 'fc4', 'ft8', 'f6', 'f2', 'af4', 'af8', 'empty', 'ekg', 'audiooutput']\n",
      "Dataset columns: ['GROUP', 'BDI', 'AUDIT', 'fp1', 'fz', 'f3', 'f7', 'below_eye', 'fc5', 'fc1', 'c3', 't7', 'left_mastoid', 'cp5', 'cp1', 'pz', 'p3', 'p7', 'o1', 'oz', 'o2', 'p4', 'p8', 'right_mastoid', 'cp6', 'cp2', 'cz', 'c4', 't8', 'above_eye', 'fc6', 'fc2', 'f4', 'f8', 'fp2', 'af7', 'af3', 'afz', 'f1', 'f5', 'ft7', 'fc3', 'fcz', 'c1', 'c5', 'tp7', 'cp3', 'p1', 'p5', 'po7', 'po3', 'poz', 'po4', 'po8', 'p6', 'p2', 'cp4', 'tp8', 'c6', 'c2', 'fc4', 'ft8', 'f6', 'f2', 'af4', 'af8', 'empty', 'ekg', 'audiooutput']\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected features:\", xgb_model.feature_names)\n",
    "print(\"Dataset columns:\", eeg_state_cleaned.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names=list(map(str,le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================Classification Report======================\n",
      "Class             Precision      Recall    F1-Score     Support\n",
      "=================================================================\n",
      "ALC                  1.0000      1.0000      1.0000           5\n",
      "CTL                  1.0000      1.0000      1.0000           5\n",
      "=================================================================\n",
      "Average F1-Score for classes : 1.0000\n",
      "=================================================================\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Generate and print classification report\n",
    "report = custom_classification_report(y_test, y_pred, target_names=target_names,class_indices = [0, 1])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict the class labels\n",
    "y_pred = xgb_model.predict(test_data)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.7016\n",
      "Epoch [2/20], Loss: 0.5802\n",
      "Epoch [3/20], Loss: 0.5176\n",
      "Epoch [4/20], Loss: 0.5567\n",
      "Epoch [5/20], Loss: 0.5143\n",
      "Epoch [6/20], Loss: 0.4184\n",
      "Epoch [7/20], Loss: 0.5059\n",
      "Epoch [8/20], Loss: 0.4789\n",
      "Epoch [9/20], Loss: 0.3400\n",
      "Epoch [10/20], Loss: 0.3436\n",
      "Epoch [11/20], Loss: 0.1569\n",
      "Epoch [12/20], Loss: 0.1748\n",
      "Epoch [13/20], Loss: 0.2781\n",
      "Epoch [14/20], Loss: 0.2094\n",
      "Epoch [15/20], Loss: 0.0824\n",
      "Epoch [16/20], Loss: 0.0391\n",
      "Epoch [17/20], Loss: 0.2546\n",
      "Epoch [18/20], Loss: 0.0803\n",
      "Epoch [19/20], Loss: 0.0245\n",
      "Epoch [20/20], Loss: 0.0858\n",
      "F1 Score: 1.0000\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 55, 128)  # Adjust the input size based on the output of conv2\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare the data\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNModel()\n",
    "\n",
    "# Calculate the correct input size for fc1\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.zeros(1, 1, X_train.shape[1])\n",
    "    dummy_output = model.conv1(dummy_input)\n",
    "    dummy_output = model.maxpool(dummy_output)\n",
    "    dummy_output = model.conv2(dummy_output)\n",
    "    dummy_output = model.maxpool(dummy_output)\n",
    "    fc1_input_size = dummy_output.view(dummy_output.size(0), -1).size(1)\n",
    "\n",
    "model.fc1 = nn.Linear(fc1_input_size, 128)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "# Calculate F1 score and accuracy\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6957\n",
      "Epoch [2/20], Loss: 0.6681\n",
      "Epoch [3/20], Loss: 0.6469\n",
      "Epoch [4/20], Loss: 0.6059\n",
      "Epoch [5/20], Loss: 0.6308\n",
      "Epoch [6/20], Loss: 0.5791\n",
      "Epoch [7/20], Loss: 0.5253\n",
      "Epoch [8/20], Loss: 0.3532\n",
      "Epoch [9/20], Loss: 0.3425\n",
      "Epoch [10/20], Loss: 0.6019\n",
      "Epoch [11/20], Loss: 0.2624\n",
      "Epoch [12/20], Loss: 0.3619\n",
      "Epoch [13/20], Loss: 0.4226\n",
      "Epoch [14/20], Loss: 0.1423\n",
      "Epoch [15/20], Loss: 0.1263\n",
      "Epoch [16/20], Loss: 0.0894\n",
      "Epoch [17/20], Loss: 0.2016\n",
      "Epoch [18/20], Loss: 0.1925\n",
      "Epoch [19/20], Loss: 0.1202\n",
      "Epoch [20/20], Loss: 0.2403\n",
      "F1 Score: 1.0000\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x.unsqueeze(1), (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Prepare the data\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "# Calculate F1 score and accuracy\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.7044\n",
      "Epoch [2/20], Loss: 0.6931\n",
      "Epoch [3/20], Loss: 0.7022\n",
      "Epoch [4/20], Loss: 0.6910\n",
      "Epoch [5/20], Loss: 0.6942\n",
      "Epoch [6/20], Loss: 0.6867\n",
      "Epoch [7/20], Loss: 0.6900\n",
      "Epoch [8/20], Loss: 0.6869\n",
      "Epoch [9/20], Loss: 0.6846\n",
      "Epoch [10/20], Loss: 0.6933\n",
      "Epoch [11/20], Loss: 0.6899\n",
      "Epoch [12/20], Loss: 0.5781\n",
      "Epoch [13/20], Loss: 0.4584\n",
      "Epoch [14/20], Loss: 0.2363\n",
      "Epoch [15/20], Loss: 0.0971\n",
      "Epoch [16/20], Loss: 1.0972\n",
      "Epoch [17/20], Loss: 0.5192\n",
      "Epoch [18/20], Loss: 0.4819\n",
      "Epoch [19/20], Loss: 0.0526\n",
      "Epoch [20/20], Loss: 0.0351\n",
      "F1 Score: 1.0000\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the CNN+LSTM model\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(32, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "# Prepare the data\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "\n",
    "model = CNNLSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "# Calculate F1 score and accuracy\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
